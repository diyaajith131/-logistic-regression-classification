# Logistic Regression Classifier

This project demonstrates **binary classification** using **Logistic Regression** on the Breast Cancer Wisconsin dataset.

## ğŸ“Œ Project Objective
- Train a logistic regression classifier.
- Evaluate using confusion matrix, precision, recall, ROC-AUC.
- Understand sigmoid function and threshold tuning.

## ğŸ“Š Dataset
We use the **Breast Cancer Wisconsin Dataset** from UCI ML Repository / Kaggle.  
[Dataset Link](https://www.kaggle.com/datasets/uciml/breast-cancer-wisconsin-data)

## âš™ï¸ Tools & Libraries
- Python 3.x
- Scikit-learn
- Pandas
- Numpy
- Matplotlib
- Seaborn

## ğŸ“‚ Repository Structure
```
logistic-regression-classifier/
â”‚â”€â”€ README.md
â”‚â”€â”€ requirements.txt
â”‚â”€â”€ data/
â”‚    â””â”€â”€ breast_cancer.csv
â”‚â”€â”€ src/
â”‚    â””â”€â”€ logistic_regression.py
â”‚â”€â”€ report/
â”‚    â””â”€â”€ report.md
â”‚â”€â”€ models/
â”‚    â””â”€â”€ model.pkl
â”‚â”€â”€ results/
â”‚    â”œâ”€â”€ confusion_matrix.png
â”‚    â”œâ”€â”€ roc_curve.png
â”‚    â””â”€â”€ metrics.txt
```

## ğŸš€ How to Run
1. Clone this repo:
   ```bash
   git clone <your_repo_link>
   cd logistic-regression-classifier
   ```
2. Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```
3. Run the training script:
   ```bash
   python src/logistic_regression.py
   ```

## ğŸ“ˆ Model Evaluation
- Confusion Matrix
- Precision, Recall, F1-score
- ROC-AUC curve

## ğŸ“– Learning Outcomes
- Logistic Regression basics
- Binary classification workflow
- Threshold tuning
- Model evaluation metrics

---
Developed as part of **AI & ML Internship Task 4**.


## ğŸ’¡ Interview Questions & Answers

**1. How does logistic regression differ from linear regression?**  
- Linear regression predicts **continuous values**.  
- Logistic regression predicts **probabilities** for classification.  

**2. What is the sigmoid function?**  
- Maps input values to range (0,1).  
- Formula: Ïƒ(z) = 1 / (1 + e^(-z))  
- Converts linear output into probability.  

**3. What is precision vs recall?**  
- **Precision** = TP / (TP + FP) â†’ Correct positive predictions.  
- **Recall** = TP / (TP + FN) â†’ Correctly identified positives.  

**4. What is the ROC-AUC curve?**  
- **ROC Curve** shows TPR vs FPR.  
- **AUC** measures performance (closer to 1 is better).  

**5. What is the confusion matrix?**  
- A 2x2 summary of predictions:  
  - TP, TN, FP, FN.  

**6. What happens if classes are imbalanced?**  
- Model may favor majority class.  
- Accuracy misleading â†’ Use **Precision, Recall, F1, AUC**.  
- Fix with oversampling, undersampling, or class weights.  

**7. How do you choose the threshold?**  
- Default = 0.5.  
- Lower threshold â†’ higher recall.  
- Higher threshold â†’ higher precision.  

**8. Can logistic regression be used for multi-class problems?**  
- Yes, using:  
  - One-vs-Rest (OvR).  
  - Multinomial (Softmax).  
